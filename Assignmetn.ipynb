{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ec64621",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fe5450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sheth\\OneDrive\\Desktop\\Fetch Assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class SentenceTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a pretrained encoder (default: bert‑base‑uncased) with mean‑pooling\n",
    "    to produce one fixed‑length embedding per sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        trainable: bool = False,      # Set True to fine‑tune\n",
    "        device: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "        # Optionally freeze the backbone\n",
    "        if not trainable:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward(self, sentences: List[str]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns (batch_size, hidden) tensor of sentence embeddings.\n",
    "        \"\"\"\n",
    "        encoded = self.tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Encoder outputs: last_hidden_state shape = (B, L, H)\n",
    "        last_hidden = self.encoder(**encoded).last_hidden_state\n",
    "\n",
    "        # --- Mean Pooling ---\n",
    "        # Mask padding tokens before averaging\n",
    "        attention_mask = encoded[\"attention_mask\"].unsqueeze(-1)  # (B, L, 1)\n",
    "        masked_hidden = last_hidden * attention_mask\n",
    "        summed = masked_hidden.sum(dim=1)              # (B, H)\n",
    "        counts = attention_mask.sum(dim=1)             # (B, 1)\n",
    "        sentence_embeddings = summed / counts.clamp(min=1e-9)\n",
    "\n",
    "        return sentence_embeddings\n",
    "\n",
    "    # Convenience encode wrapper\n",
    "    @torch.inference_mode()\n",
    "    def encode(self, sentences: List[str]) -> torch.Tensor:\n",
    "        return self.forward(sentences).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dd9837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 768])\n",
      "tensor([[ 0.2334,  0.1515,  0.0473,  ..., -0.1749, -0.2418,  0.0309],\n",
      "        [-0.1025, -0.3769,  0.0619,  ...,  0.0876, -0.7424,  0.5973],\n",
      "        [-0.0817, -0.3850,  0.0510,  ...,  0.0072,  0.0259, -0.0114]])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = SentenceTransformer(trainable=False)\n",
    "    samples = [\n",
    "        \"Transformers are changing natural‑language processing.\",\n",
    "        \"An embedding represents a sentence as a dense vector.\",\n",
    "        \"The weather is lovely today!\",\n",
    "    ]\n",
    "    embeddings = model.encode(samples)\n",
    "    print(\"Embeddings shape:\", embeddings.shape)  # (3, 768)\n",
    "    print(embeddings)  # full tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050d619",
   "metadata": {},
   "source": [
    "#### Model Architecture Choices\n",
    "\n",
    "Backbone Model:\n",
    " - Used bert-base-uncased from Hugging Face.\n",
    " - Chosen for its balance of performance and efficiency, widely adopted and well-supported.\n",
    "\n",
    "Embedding Strategy:\n",
    " - Used mean pooling over the last hidden layer.\n",
    " - Applied attention mask to exclude padded tokens during pooling.\n",
    " - Chosen for simplicity and effectiveness across unsupervised and similarity tasks.\n",
    "\n",
    "Trainability Configuration:\n",
    " - Included a trainable flag to toggle freezing of the backbone.\n",
    " - Allows use as a static feature extractor or as a fine-tunable encoder.\n",
    "\n",
    "Device Handling:\n",
    " - Automatically detects GPU with torch.cuda.is_available().\n",
    " - Ensures code runs efficiently on both CPU and GPU without manual changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100618f9",
   "metadata": {},
   "source": [
    "#### Implementation Choices\n",
    "\n",
    "Frameworks Used:\n",
    " - PyTorch: for neural network modules and training.\n",
    " - Transformers (Hugging Face): for pretrained models and tokenization.\n",
    "\n",
    "Tokenizer Settings:\n",
    " - padding=True, truncation=True ensures uniform input size in batches.\n",
    " - Returns PyTorch tensors for seamless integration with the model.\n",
    "\n",
    "Model Structure:\n",
    " - Single class SentenceTransformer encapsulates tokenizer, encoder, and pooling.\n",
    " - Clean separation of components allows easy extension to multi-task learning.\n",
    "\n",
    "Inference Convenience:\n",
    " - Provided .encode() method with @torch.inference_mode() for easy use and no gradient computation.\n",
    "\n",
    "Testing:\n",
    " - Included three example sentences to demonstrate sentence embedding outputs.\n",
    " - Printed embedding shape and actual tensor values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25058e2",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de8a4a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder with two parallel heads:\n",
    "      • Task A – sentence‑level classification\n",
    "      • Task B – token‑level classification (e.g., NER)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        num_classes_task_a: int = 4,\n",
    "        num_labels_task_b: int = 7,\n",
    "        trainable: bool = True,\n",
    "        device: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared backbone\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "\n",
    "        # Task heads\n",
    "        self.sent_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_classes_task_a),\n",
    "        )\n",
    "        self.token_classifier = nn.Linear(hidden_size, num_labels_task_b)\n",
    "\n",
    "        # Optional freezing\n",
    "        if not trainable:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        # Device\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    # --------------------------------------------------- #\n",
    "    #  Training / inference forward (tensor interface)   #\n",
    "    # --------------------------------------------------- #\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "        input_ids:      (B, L)\n",
    "        attention_mask: (B, L)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict with:\n",
    "            • task_a_logits: (B, num_classes_task_a)\n",
    "            • task_b_logits: (B, L, num_labels_task_b)\n",
    "        \"\"\"\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state                          # (B, L, H)\n",
    "\n",
    "        # Task A\n",
    "        mask = attention_mask.unsqueeze(-1)                              # (B, L, 1)\n",
    "        pooled = (last_hidden * mask).sum(1) / mask.sum(1).clamp(min=1e-9)\n",
    "        task_a_logits = self.sent_classifier(pooled)                     # (B, C)\n",
    "\n",
    "        # Task B\n",
    "        task_b_logits = self.token_classifier(last_hidden)               # (B, L, K)\n",
    "\n",
    "        return {\"task_a_logits\": task_a_logits, \"task_b_logits\": task_b_logits}\n",
    "\n",
    "    # --------------------------------------------------- #\n",
    "    #  Convenience wrapper (raw sentences → predictions)  #\n",
    "    # --------------------------------------------------- #\n",
    "    @torch.inference_mode()\n",
    "    def predict(self, sentences: List[str]):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        tuple (cls_ids, ner_ids):\n",
    "            cls_ids: Tensor (B,)   – predicted class index per sentence\n",
    "            ner_ids: Tensor (B, L) – predicted label index per token\n",
    "        \"\"\"\n",
    "        enc = self.tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        out = self.forward(enc[\"input_ids\"], enc[\"attention_mask\"])\n",
    "        cls_ids = out[\"task_a_logits\"].argmax(-1).cpu()\n",
    "        ner_ids = out[\"task_b_logits\"].argmax(-1).cpu()\n",
    "        return cls_ids, ner_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db06980a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task A - predicted class indices: [2, 2]\n",
      "Task B - predicted NER label indices (per token):\n",
      "  Barack Obama was born in Hawaii.\n",
      "  [2, 6, 6, 2, 2, 3, 3, 3, 3, 6, 6, 5]\n",
      "  Apple unveiled a new iPhone during its September event.\n",
      "  [2, 2, 3, 2, 2, 1, 5, 3, 3, 3, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = MultiTaskSentenceTransformer(trainable=False)\n",
    "\n",
    "    demo_sentences = [\n",
    "        \"Barack Obama was born in Hawaii.\",\n",
    "        \"Apple unveiled a new iPhone during its September event.\",\n",
    "    ]\n",
    "    cls_ids, ner_ids = model.predict(demo_sentences)\n",
    "\n",
    "    print(\"Task A - predicted class indices:\", cls_ids.tolist())\n",
    "    print(\"Task B - predicted NER label indices (per token):\")\n",
    "    for sent, ids in zip(demo_sentences, ner_ids):\n",
    "        print(f\"  {sent}\\n  {ids.tolist()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38755b45",
   "metadata": {},
   "source": [
    "#### Architectural changes for multi‑task learning:\n",
    "\n",
    "Shared Transformer Encoder:\n",
    "     - Kept the original bert-base-uncased backbone to provide a common semantic representation for all tasks.\n",
    "\n",
    "Task‑specific Heads:\n",
    " - Sentence Classification Head (Task A):\n",
    "    - Mean‑pooled sentence embedding → 2‑layer feed‑forward network → softmax over num_classes_task_a.\n",
    " - Token Classification Head (Task B: NER):\n",
    "    - Linear layer applied to every token hidden state → softmax over num_labels_task_b.    \n",
    "\n",
    "Output Shapes:\n",
    " - Task A logits: (batch_size, num_classes_task_a)\n",
    " - Task B logits: (batch_size, seq_len, num_labels_task_b)\n",
    "\n",
    "Loss Computation (training‑time):\n",
    " - Use cross‑entropy for both heads.\n",
    " - Total loss = λ * loss_task_a + (1‑λ) * loss_task_b, where λ can be tuned or dynamically balanced.\n",
    "\n",
    "Gradient Flow:\n",
    " - Both heads back‑propagate through the shared encoder, enabling transfer of useful features across tasks.\n",
    "\n",
    "Data Handling:\n",
    " - Sentences are tokenized once; the same batch feeds both heads.\n",
    " - For NER, generated label IDs must align with tokenized sub‑words (e.g., first‑sub‑token labelling or BIO on split tokens).\n",
    "\n",
    "Flexibility:\n",
    " - Heads are independent modules—additional tasks (e.g., sentence similarity regression, question answering) can be appended with minimal code changes.\n",
    "\n",
    "Training Strategy:\n",
    " - Alternate mini‑batches from each task or mix them within a batch.\n",
    " - Optionally freeze encoder for initial steps, then unfreeze for full fine‑tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ebcbb",
   "metadata": {},
   "source": [
    "Why the numbers vary every run\n",
    "The BERT backbone is pretrained and sensible, but both task‑specific heads start with random weights (they have never been trained), so the logits—and therefore the arg‑max IDs—are effectively random.\n",
    "\n",
    "If you run the script again, you’ll almost certainly see different class IDs and token‑label IDs unless you set a manual random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afae5b2",
   "metadata": {},
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640dc950",
   "metadata": {},
   "source": [
    "### Training‑time configuration options\n",
    "#### 1  Entire network frozen\n",
    "What happens\n",
    " - Backbone and all task‑specific heads keep their initial weights.\n",
    " - The model becomes a static feature extractor; gradients are never computed.\n",
    "\n",
    "Implications\n",
    " - Fastest training ‑‑ forward pass only.\n",
    " - Zero risk of over‑fitting because nothing changes.\n",
    " - Works only if the heads are already well‑trained or if you care strictly about embedding quality for downstream nearest‑neighbour / clustering tasks.\n",
    "\n",
    "When it makes sense\n",
    " - You have no labelled data but need quick sentence embeddings.\n",
    " - You are running on the edge with limited memory/compute and can’t afford back‑prop.\n",
    " - Serving latency is critical and you can pre‑compute all embeddings offline.\n",
    "\n",
    "#### 2  Only transformer backbone frozen\n",
    "What happens\n",
    " - The large pretrained encoder is locked; only the lightweight heads learn.\n",
    "\n",
    "Advantages\n",
    " - Lower memory and compute than full fine‑tuning—the backbone’s activations do not require gradient storage.\n",
    " - Reduced catastrophic forgetting: preserves the general‑language knowledge captured during pre‑training.\n",
    " - Heads adapt quickly even with a small labelled set; common in few‑shot settings.\n",
    "\n",
    "Trade‑offs\n",
    " - Can’t adapt deeper representations to the specifics of your domain (e.g. biomedical jargon).\n",
    " - If tasks differ greatly from pre‑training data, performance may be capped.\n",
    "\n",
    "Typical usage\n",
    " - Many production systems fine‑tune only classification layers while freezing BERT to save GPU hours.\n",
    " - Academic low‑resource benchmarks often start with this setup.\n",
    "\n",
    "#### 3  Freeze just one task‑specific head\n",
    "Why you might do it\n",
    " - You already have a well‑performing head for Task A but must add Task B without hurting Task A.\n",
    " - Or you’re transferring knowledge from Task A to Task B and want Task A as a regulariser.\n",
    "\n",
    "Effects\n",
    " - Gradients from the frozen head do not flow into that head’s weights, but they do flow through the shared encoder (unless it is also frozen).\n",
    " - Keeps performance on the frozen task stable while letting the other task improve.\n",
    "\n",
    "Pitfalls\n",
    " - Imbalanced optimisation dynamics—loss from the frozen head is absent, so you must monitor that the encoder doesn’t drift and degrade Task A anyway.\n",
    " - Sometimes requires loss‑weighting tricks or periodic evaluation checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da934b8",
   "metadata": {},
   "source": [
    "### Transfer learning scenario\n",
    "Choice of a pretrained model\n",
    " - Start with a domain‑general large model such as bert-base-uncased or a domain‑specific one like BioClinicalBERT if the target corpus is biomedical.\n",
    " - Criteria: vocabulary coverage, size vs. compute budget, licence compatibility.\n",
    "\n",
    "Layers to freeze or unfreeze\n",
    " - Step 1 – Warm‑up\n",
    "    - Freeze all encoder layers. Train only the new heads for a few epochs to stabilise learning signals.\n",
    " - Step 2 – Gradual unfreezing\n",
    "    - Unfreeze the top N transformer layers (closest to the output) first.\n",
    "    - Optionally continue unfreezing lower layers one block at a time (ULMFiT‑style) while reducing learning rate for deeper layers.\n",
    "\n",
    " - Optional regularisation\n",
    "    - Use layer‑wise learning‑rate decay: higher LR for task heads, progressively smaller LR for early transformer layers.\n",
    "\n",
    "Rationale\n",
    " - Preserve lower‑level lexical and syntactic features that transfer well across domains.\n",
    " - Adapt higher‑level semantic representations to domain‑specific patterns (medical entities, sentiment cues, etc.).\n",
    " - Staged unfreezing avoids large, unstable gradient updates that could destroy pretrained knowledge (catastrophic forgetting).\n",
    " - Layer‑wise LR decay mirrors the intuition that deeper layers need only a gentle nudge while heads require significant updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3299efc0",
   "metadata": {},
   "source": [
    "#### Key Decisions & Insights (Training Strategy)\n",
    "Fine‑tuning granularity\n",
    " - Keep more layers frozen when data are scarce or latency is critical; unfreeze progressively as domain shift or accuracy needs grow.\n",
    "\n",
    "Two‑stage transfer learning\n",
    " - Train task heads only to stabilise gradients.\n",
    " - Gradually unfreeze upper transformer layers while using smaller learning rates for lower layers.\n",
    "\n",
    "Layer‑wise learning‑rate decay (LLRD)\n",
    " - Apply high LR to the new heads, medium LR to upper encoder blocks, and very small LR to early blocks to avoid catastrophic forgetting.\n",
    "\n",
    "Multi‑task loss balancing\n",
    " - Combine losses as λ · L_A + (1 − λ) · L_B; tune λ or adopt dynamic weighting to prevent one task from dominating.\n",
    "\n",
    "Selective head freezing\n",
    " - Freeze a mature head (e.g., Task A) when adding a new task so its performance remains stable while the shared encoder still adapts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9239b827",
   "metadata": {},
   "source": [
    "TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaa074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Hypothetical data pipeline\n",
    "# ------------------------------\n",
    "\n",
    "SENTENCES = [\n",
    "    \"Barack Obama was born in Hawaii.\",\n",
    "    \"Apple unveiled a new iPhone.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"OpenAI released a new language model.\",\n",
    "]\n",
    "NUM_CLASSES_A = 4          # sentence‑level classes\n",
    "NUM_LABELS_B = 7           # token‑level NER labels (BIO)\n",
    "\n",
    "def random_label_a() -> int:\n",
    "    return random.randint(0, NUM_CLASSES_A - 1)\n",
    "\n",
    "def random_labels_b(n_tokens: int) -> List[int]:\n",
    "    return [random.randint(0, NUM_LABELS_B - 1) for _ in range(n_tokens)]\n",
    "\n",
    "class ToyMultiTaskDataset(Dataset):\n",
    "    def __init__(self, n_samples: int = 100):\n",
    "        self.samples = [random.choice(SENTENCES) for _ in range(n_samples)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.samples[idx]\n",
    "        # fake labels (normally read from annotation files)\n",
    "        label_a = random_label_a()\n",
    "        label_b = random_labels_b(len(sent.split()))    # crude token count\n",
    "        return {\"sentence\": sent,\n",
    "                \"label_a\": label_a,\n",
    "                \"label_b\": label_b}\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Collate‑fn for DataLoader\n",
    "# ------------------------------\n",
    "\n",
    "def collate(batch: List[Dict], tokenizer) -> Dict[str, torch.Tensor]:\n",
    "    sents  = [item[\"sentence\"]  for item in batch]\n",
    "    labelA = torch.tensor([item[\"label_a\"] for item in batch], dtype=torch.long)\n",
    "\n",
    "    # Tokenise once for both tasks\n",
    "    enc = tokenizer(\n",
    "        sents,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    seq_len = enc[\"input_ids\"].shape[1]\n",
    "\n",
    "    # Pad / truncate Task‑B labels to match WordPiece length\n",
    "    padded_b = torch.full((len(batch), seq_len),\n",
    "                          fill_value=-100, dtype=torch.long)   # ignore_index\n",
    "    for i, item in enumerate(batch):\n",
    "        n = min(seq_len, len(item[\"label_b\"]))\n",
    "        padded_b[i, :n] = torch.tensor(item[\"label_b\"][:n])\n",
    "\n",
    "    enc[\"label_a\"] = labelA\n",
    "    enc[\"label_b\"] = padded_b\n",
    "    return enc\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Training loop\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_weights: Tuple[float,float], device):\n",
    "    model.train()\n",
    "    ce_sent  = nn.CrossEntropyLoss()\n",
    "    ce_token = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        out = model.forward(batch[\"input_ids\"], batch[\"attention_mask\"])  # slight mod of forward\n",
    "\n",
    "        # ---- Task A loss ----\n",
    "        loss_a = ce_sent(out[\"task_a_logits\"], batch[\"label_a\"])\n",
    "\n",
    "        # ---- Task B loss ----\n",
    "        # out[\"task_b_logits\"]: (B, L, K)  → reshape for CE\n",
    "        token_logits = out[\"task_b_logits\"].view(-1, out[\"task_b_logits\"].size(-1))\n",
    "        token_labels = batch[\"label_b\"].view(-1)\n",
    "        loss_b = ce_token(token_logits, token_labels)\n",
    "\n",
    "        # ---- Weighted joint loss ----\n",
    "        loss = loss_weights[0] * loss_a + loss_weights[1] * loss_b\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # minimal metric: Task A accuracy\n",
    "        preds_a = out[\"task_a_logits\"].argmax(-1)\n",
    "        correct += (preds_a == batch[\"label_a\"]).sum().item()\n",
    "        total   += preds_a.size(0)\n",
    "        total_loss += loss.item() * preds_a.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Main script\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = MultiTaskSentenceTransformer(\n",
    "        num_classes_task_a=NUM_CLASSES_A,\n",
    "        num_labels_task_b=NUM_LABELS_B,\n",
    "    ).to(device)\n",
    "\n",
    "    dataset = ToyMultiTaskDataset(n_samples=256)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate(b, model.tokenizer),\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    lambda_a, lambda_b = 0.5, 0.5          # loss weights\n",
    "\n",
    "    for epoch in range(3):\n",
    "        loss, acc = train_one_epoch(\n",
    "            model,\n",
    "            loader,\n",
    "            optimizer,\n",
    "            (lambda_a, lambda_b),\n",
    "            device\n",
    "        )\n",
    "        print(f\"Epoch {epoch+1}: loss={loss:.4f} | Task-A acc={acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78e58ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=1.6803 | Task‑A acc=0.266\n",
      "Epoch 2: loss=1.6747 | Task‑A acc=0.246\n",
      "Epoch 3: loss=1.6738 | Task‑A acc=0.246\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7545d0a",
   "metadata": {},
   "source": [
    "#### Assumptions & Design Rationale\n",
    "Synthetic dataset\n",
    " - Generates random sentences and labels—just to illustrate shapes and collate logic.\n",
    " - In practice, replace with a Dataset that reads real annotated files.\n",
    "\n",
    "Single pass tokenisation\n",
    " - Collate‑fn tokenises the sentence batch once, creating input_ids & attention_mask for both tasks.\n",
    "\n",
    "Label padding\n",
    " - Token‑level labels are padded to the max sequence length with -100; nn.CrossEntropyLoss(ignore_index=-100) skips them.\n",
    "\n",
    "Weighted joint loss\n",
    " - Two cross‑entropy losses combined as λA·L_A + λB·L_B.\n",
    " - Scalars λ adjust task importance; tune on a validation set.\n",
    "\n",
    "Metrics\n",
    " - For brevity, only sentence‑level accuracy (Task A) is computed.\n",
    " - Token‑level F1 or seqeval metrics can be added similarly—update after each batch or epoch.\n",
    "\n",
    "Optimiser & LR\n",
    " - Uses AdamW with a single learning rate; real training might employ layer‑wise decay or scheduler.\n",
    "\n",
    "Epoch loop\n",
    " - Shows three toy epochs and prints running loss and Task A accuracy to confirm the loop executes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0f38f",
   "metadata": {},
   "source": [
    "#### How Multi‑Task Training Operates Here\n",
    "Shared Forward Pass\n",
    " - The batch is encoded once by the transformer; hidden states feed both heads, saving compute.\n",
    "\n",
    "Two Losses, Shared Gradient Flow\n",
    " - Gradients from both heads propagate through the shared backbone (unless frozen), enabling cross‑task feature sharing.\n",
    "\n",
    "Balancing Tasks\n",
    " - Loss weights λ counteract dataset‑size or scale imbalance.\n",
    " - Alternative schemes: dynamic uncertainty weighting or alternating mini‑batches per task.\n",
    "\n",
    "Metrics Isolation\n",
    " - Each task keeps its own validation metrics—even if the losses are blended—so progress can be monitored independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77efa44",
   "metadata": {},
   "source": [
    "#### Key Decisions & Insights (Training Loop)\n",
    "Tensor‑based forward interface\n",
    " - Accepts input_ids and attention_mask to ensure one tokenisation per batch and seamless mixed‑precision or distributed training.\n",
    "\n",
    "Custom collate_fn\n",
    " - Pads token‑level labels with ‑100 and creates the attention mask, preparing a batch that both heads can consume.\n",
    "\n",
    "Joint loss computation\n",
    " - Uses cross‑entropy for each task; total loss is a weighted sum, making it easy to adjust task importance.\n",
    "\n",
    "Metric tracking\n",
    " - Computes Task A sentence‑level accuracy each batch; leaves hooks for token‑level F1 so each task can be monitored independently.\n",
    "\n",
    "Optimiser and learning rate\n",
    " - Starts with AdamW at 2 × 10⁻⁵; ready to swap in LLRD or schedulers once encoder layers are unfrozen.\n",
    "\n",
    "Scalability hooks\n",
    " - Automatic device selection, batch‑to‑device mapping, and a modular dataset class keep the loop runnable on CPU, single GPU, or multi‑GPU frameworks.\n",
    "\n",
    "Progressive workflow\n",
    " - Run heads‑only training for quick convergence.\n",
    " - Unfreeze additional layers as validation metrics plateau.\n",
    " - Tweak λ weights and learning rates to maintain balance between tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8caf175",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
